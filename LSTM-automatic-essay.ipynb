{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.read_csv(os.path.join('./data/', 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only need essay id essay_set essay domain1_score\n",
    "# removing rater1_domain1   , rater2_domain1\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n",
    "# removing NaN values\n",
    "\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         8\n",
       "1         9\n",
       "2         7\n",
       "3        10\n",
       "4         8\n",
       "         ..\n",
       "12971    35\n",
       "12972    32\n",
       "12973    40\n",
       "12974    40\n",
       "12975    40\n",
       "Name: domain1_score, Length: 12976, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading y values to y\n",
    "y = X['domain1_score']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'essay_set', 'essay', 'domain1_score'], dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of essay set :  8\n",
      "\n",
      "----------total essays in each set--------\n",
      "    essay_set  size\n",
      "0          1  1783\n",
      "1          2  1800\n",
      "2          3  1726\n",
      "3          4  1770\n",
      "4          5  1805\n",
      "5          6  1800\n",
      "6          7  1569\n",
      "7          8   723 \n",
      "\n",
      "total :  12976\n"
     ]
    }
   ],
   "source": [
    "# number of essay set\n",
    "print('number of essay set : ',len(X['essay_set'].unique()))\n",
    "# number of essays in each set\n",
    "n_essay = X.groupby(['essay_set'], as_index=False)['domain1_score'].size()\n",
    "print('\\n----------total essays in each set--------\\n',n_essay,'\\n\\ntotal : ', n_essay['size'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------max score of each set--------\n",
      "    essay_set  domain1_score\n",
      "0          1             12\n",
      "1          2              6\n",
      "2          3              3\n",
      "3          4              3\n",
      "4          5              4\n",
      "5          6              4\n",
      "6          7             24\n",
      "7          8             60 \n",
      "\n",
      " \n",
      "----------min score of each set--------\n",
      "    essay_set  domain1_score\n",
      "0          1              2\n",
      "1          2              1\n",
      "2          3              0\n",
      "3          4              0\n",
      "4          5              0\n",
      "5          6              0\n",
      "6          7              2\n",
      "7          8             10\n"
     ]
    }
   ],
   "source": [
    "# max and min score of each set\n",
    "max_score = X.groupby(['essay_set'], as_index=False)['domain1_score'].max()\n",
    "min_score = X.groupby(['essay_set'], as_index=False)['domain1_score'].min()\n",
    "print('\\n----------max score of each set--------\\n',max_score,'\\n\\n','\\n----------min score of each set--------\\n', min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "\n",
    "2 LSTM 1 Drop out 1 Dense\n",
    "\n",
    "LSTM 1 300 hidden units\n",
    "\n",
    "LSTM 2 64 hidden units\n",
    "\n",
    "Dropout layer  0.5 dropout rate\n",
    "\n",
    "Dense activation relu\n",
    "\n",
    "loss = MSE and optimizer = rmsprop (0.9 default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    # 300 hidden units\n",
    "    model.add(LSTM(300, dropout=0.5, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    # 64 hidden units\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    #0.5 dropout rate\n",
    "    model.add(Dropout(0.5))\n",
    "    # activaion relu\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    #rmsprop\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essay embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_essay(essay_v):\n",
    "    \n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    # making lower case\n",
    "    words = essay_v.lower()\n",
    "    words = words.split()\n",
    "    len_words = len(words)\n",
    "    # removing stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    if(len(words)>0):\n",
    "        l = len(words)\n",
    "        #print(l)\n",
    "    return (words)\n",
    "\n",
    "def senteces_list(essay_v):\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    #print(raw_sentences)\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        sent = tokenize_essay(raw_sentence)\n",
    "        if len(sent) > 0:\n",
    "            sentences.append(sent)\n",
    "        else:\n",
    "            warning_user = \"no words in sent\"\n",
    "            #print(warning_user)\n",
    "    return sentences\n",
    "\n",
    "# referece from word2vec stackoverflow\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "   # crating feature vec \n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])  \n",
    "    # getting average of sentence vec      \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold for final report\n",
    "This is a training with single test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#creating test and train essays\n",
    "train_essays = X_train['essay']\n",
    "test_essays = X_test['essay']\n",
    "\n",
    "# len of train and test essay\n",
    "len_train_essays = len(train_essays)\n",
    "len_test_essays = len(test_essays)\n",
    "#print(len_train_essays,len_test_essays)\n",
    "\n",
    "# spliting into sentences \n",
    "sentences = []\n",
    "for essay in train_essays:\n",
    "            sentences += senteces_list(essay)\n",
    "            \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer\n",
    "\n",
    "word2vec embedding acts as embedding layer\n",
    "\n",
    "300 feature vectors are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/fprdqqg93d3cp30hvc240yqr0000gn/T/ipykernel_44021/2316545788.py:4: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "# creating a word2 vec model\n",
    "model = Word2Vec(sentences, workers=4, vector_size=300, min_count = 40, window = 10, sample = 1e-3)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "#from word2 vec creating  embedding\n",
    "clean_train_essays = []\n",
    "    \n",
    "\n",
    "for essay_v in train_essays:\n",
    "    clean_train_essays.append(tokenize_essay(essay_v))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, 300)\n",
    "    \n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(tokenize_essay( essay_v))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, 300 )\n",
    "    \n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "Training model\n",
      "Epoch 1/150\n",
      "136/136 [==============================] - 5s 15ms/step - loss: 68.2200 - mae: 4.3279\n",
      "Epoch 2/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 39.9629 - mae: 3.4785\n",
      "Epoch 3/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 32.2575 - mae: 3.2785\n",
      "Epoch 4/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 28.0136 - mae: 3.1316\n",
      "Epoch 5/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 25.3539 - mae: 2.9663\n",
      "Epoch 6/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 23.1706 - mae: 2.8037\n",
      "Epoch 7/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 21.3353 - mae: 2.6566\n",
      "Epoch 8/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 19.3351 - mae: 2.5174\n",
      "Epoch 9/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 17.3877 - mae: 2.3756\n",
      "Epoch 10/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 15.2431 - mae: 2.2391\n",
      "Epoch 11/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 13.9833 - mae: 2.1449\n",
      "Epoch 12/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 13.5208 - mae: 2.1035\n",
      "Epoch 13/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 13.1959 - mae: 2.0595\n",
      "Epoch 14/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 12.6961 - mae: 2.0160\n",
      "Epoch 15/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 11.8404 - mae: 1.9744\n",
      "Epoch 16/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 11.5028 - mae: 1.9247\n",
      "Epoch 17/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 10.9156 - mae: 1.8942\n",
      "Epoch 18/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 10.9064 - mae: 1.8969\n",
      "Epoch 19/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 10.4604 - mae: 1.8465\n",
      "Epoch 20/150\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 10.6534 - mae: 1.8394\n",
      "Epoch 21/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 10.4231 - mae: 1.8393\n",
      "Epoch 22/150\n",
      "136/136 [==============================] - 3s 19ms/step - loss: 10.5203 - mae: 1.8182\n",
      "Epoch 23/150\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 9.8582 - mae: 1.7738\n",
      "Epoch 24/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 9.7935 - mae: 1.7543\n",
      "Epoch 25/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 9.9221 - mae: 1.7474\n",
      "Epoch 26/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 9.4109 - mae: 1.7378\n",
      "Epoch 27/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 9.5131 - mae: 1.7071\n",
      "Epoch 28/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 9.0200 - mae: 1.6768\n",
      "Epoch 29/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 9.9065 - mae: 1.7448\n",
      "Epoch 30/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 9.1211 - mae: 1.6764\n",
      "Epoch 31/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 8.7939 - mae: 1.6681\n",
      "Epoch 32/150\n",
      "136/136 [==============================] - 3s 19ms/step - loss: 8.5401 - mae: 1.6493\n",
      "Epoch 33/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 8.5670 - mae: 1.6406\n",
      "Epoch 34/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 8.7126 - mae: 1.6438\n",
      "Epoch 35/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 8.6926 - mae: 1.6504\n",
      "Epoch 36/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 8.6911 - mae: 1.6535\n",
      "Epoch 37/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 8.2700 - mae: 1.6200\n",
      "Epoch 38/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 8.3216 - mae: 1.6301\n",
      "Epoch 39/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.9987 - mae: 1.6002\n",
      "Epoch 40/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 8.1052 - mae: 1.6029\n",
      "Epoch 41/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 7.7749 - mae: 1.5934\n",
      "Epoch 42/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 7.7279 - mae: 1.5867\n",
      "Epoch 43/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 8.1681 - mae: 1.6066\n",
      "Epoch 44/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.7546 - mae: 1.5736\n",
      "Epoch 45/150\n",
      "136/136 [==============================] - 3s 19ms/step - loss: 8.0901 - mae: 1.5919\n",
      "Epoch 46/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 7.8113 - mae: 1.5900\n",
      "Epoch 47/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.7621 - mae: 1.5644\n",
      "Epoch 48/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 7.6998 - mae: 1.5748\n",
      "Epoch 49/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 8.1024 - mae: 1.5731\n",
      "Epoch 50/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 7.4550 - mae: 1.5394\n",
      "Epoch 51/150\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 7.2085 - mae: 1.5335\n",
      "Epoch 52/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 7.6135 - mae: 1.5530\n",
      "Epoch 53/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 7.3748 - mae: 1.5313\n",
      "Epoch 54/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.6642 - mae: 1.5451\n",
      "Epoch 55/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 7.3120 - mae: 1.5286\n",
      "Epoch 56/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.6922 - mae: 1.4934\n",
      "Epoch 57/150\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 7.4670 - mae: 1.5170\n",
      "Epoch 58/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 7.5820 - mae: 1.5341\n",
      "Epoch 59/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 7.1580 - mae: 1.5100\n",
      "Epoch 60/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 7.3544 - mae: 1.5137\n",
      "Epoch 61/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 7.2906 - mae: 1.5039\n",
      "Epoch 62/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 7.2751 - mae: 1.5028\n",
      "Epoch 63/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.1785 - mae: 1.4970\n",
      "Epoch 64/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.3268 - mae: 1.4942\n",
      "Epoch 65/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 7.0739 - mae: 1.4874\n",
      "Epoch 66/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 7.0676 - mae: 1.4846\n",
      "Epoch 67/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 7.1152 - mae: 1.4944\n",
      "Epoch 68/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.5280 - mae: 1.4458\n",
      "Epoch 69/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.5698 - mae: 1.4532\n",
      "Epoch 70/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 7.0476 - mae: 1.4771\n",
      "Epoch 71/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.8380 - mae: 1.4725\n",
      "Epoch 72/150\n",
      "136/136 [==============================] - 3s 19ms/step - loss: 6.7162 - mae: 1.4479\n",
      "Epoch 73/150\n",
      "136/136 [==============================] - 3s 19ms/step - loss: 7.2836 - mae: 1.4891\n",
      "Epoch 74/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 6.6117 - mae: 1.4685\n",
      "Epoch 75/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.6185 - mae: 1.4630\n",
      "Epoch 76/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.9767 - mae: 1.4773\n",
      "Epoch 77/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.9250 - mae: 1.4656\n",
      "Epoch 78/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.5358 - mae: 1.4430\n",
      "Epoch 79/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.7547 - mae: 1.4564\n",
      "Epoch 80/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.6401 - mae: 1.4376\n",
      "Epoch 81/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.4541 - mae: 1.4317\n",
      "Epoch 82/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.7508 - mae: 1.4575\n",
      "Epoch 83/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 6.7519 - mae: 1.4441\n",
      "Epoch 84/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.9182 - mae: 1.4681\n",
      "Epoch 85/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.2928 - mae: 1.4114\n",
      "Epoch 86/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.7298 - mae: 1.4412\n",
      "Epoch 87/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.7667 - mae: 1.4503\n",
      "Epoch 88/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.2352 - mae: 1.4076\n",
      "Epoch 89/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 6.2919 - mae: 1.4192\n",
      "Epoch 90/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 6.9623 - mae: 1.4427\n",
      "Epoch 91/150\n",
      "136/136 [==============================] - 2s 11ms/step - loss: 6.4847 - mae: 1.4231\n",
      "Epoch 92/150\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 6.2764 - mae: 1.4003\n",
      "Epoch 93/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.4778 - mae: 1.4241\n",
      "Epoch 94/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.6355 - mae: 1.4281\n",
      "Epoch 95/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.1495 - mae: 1.3933\n",
      "Epoch 96/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.6892 - mae: 1.4193\n",
      "Epoch 97/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.4405 - mae: 1.4208\n",
      "Epoch 98/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.7606 - mae: 1.4346\n",
      "Epoch 99/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.6814 - mae: 1.4383\n",
      "Epoch 100/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.4912 - mae: 1.4230\n",
      "Epoch 101/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.5528 - mae: 1.4254\n",
      "Epoch 102/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.5539 - mae: 1.4243\n",
      "Epoch 103/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 6.2351 - mae: 1.4156\n",
      "Epoch 104/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.1690 - mae: 1.3858\n",
      "Epoch 105/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.0353 - mae: 1.3822\n",
      "Epoch 106/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.3672 - mae: 1.4023\n",
      "Epoch 107/150\n",
      "136/136 [==============================] - 3s 18ms/step - loss: 6.4829 - mae: 1.4181\n",
      "Epoch 108/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.1765 - mae: 1.4055\n",
      "Epoch 109/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.5514 - mae: 1.4132\n",
      "Epoch 110/150\n",
      "136/136 [==============================] - 2s 18ms/step - loss: 5.9747 - mae: 1.3807\n",
      "Epoch 111/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.4271 - mae: 1.4094\n",
      "Epoch 112/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.5350 - mae: 1.4164\n",
      "Epoch 113/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.1803 - mae: 1.3898\n",
      "Epoch 114/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.6208 - mae: 1.4246\n",
      "Epoch 115/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.3590 - mae: 1.4142\n",
      "Epoch 116/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.1062 - mae: 1.3912\n",
      "Epoch 117/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.3055 - mae: 1.3938\n",
      "Epoch 118/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.4914 - mae: 1.3998\n",
      "Epoch 119/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.4700 - mae: 1.4049\n",
      "Epoch 120/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.3435 - mae: 1.4027\n",
      "Epoch 121/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.1825 - mae: 1.3949\n",
      "Epoch 122/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.3992 - mae: 1.4022\n",
      "Epoch 123/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.4442 - mae: 1.3993\n",
      "Epoch 124/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.3454 - mae: 1.3887\n",
      "Epoch 125/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.0365 - mae: 1.3917\n",
      "Epoch 126/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.4052 - mae: 1.3913\n",
      "Epoch 127/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.4390 - mae: 1.3956\n",
      "Epoch 128/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.4687 - mae: 1.3949\n",
      "Epoch 129/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 5.8688 - mae: 1.3479\n",
      "Epoch 130/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.3129 - mae: 1.3983\n",
      "Epoch 131/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 5.8019 - mae: 1.3552\n",
      "Epoch 132/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.2611 - mae: 1.3941\n",
      "Epoch 133/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 5.8514 - mae: 1.3698\n",
      "Epoch 134/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.1089 - mae: 1.3783\n",
      "Epoch 135/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 5.9489 - mae: 1.3640\n",
      "Epoch 136/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.1869 - mae: 1.3887\n",
      "Epoch 137/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.3083 - mae: 1.3981\n",
      "Epoch 138/150\n",
      "136/136 [==============================] - 2s 16ms/step - loss: 6.0718 - mae: 1.3678\n",
      "Epoch 139/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 6.0286 - mae: 1.3818\n",
      "Epoch 140/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 6.2156 - mae: 1.3842\n",
      "Epoch 141/150\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 6.1685 - mae: 1.3837\n",
      "Epoch 142/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 6.0548 - mae: 1.3779\n",
      "Epoch 143/150\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 5.9957 - mae: 1.3717\n",
      "Epoch 144/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 6.1796 - mae: 1.3768\n",
      "Epoch 145/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 5.8860 - mae: 1.3463\n",
      "Epoch 146/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 5.8698 - mae: 1.3519\n",
      "Epoch 147/150\n",
      "136/136 [==============================] - 2s 17ms/step - loss: 5.8667 - mae: 1.3657\n",
      "Epoch 148/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.0553 - mae: 1.3597\n",
      "Epoch 149/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.0331 - mae: 1.3772\n",
      "Epoch 150/150\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 6.4250 - mae: 1.3859\n",
      "Traiing completed\n"
     ]
    }
   ],
   "source": [
    "train_shape = trainDataVecs.shape\n",
    "test_shape = testDataVecs.shape\n",
    "#print(train_shape,test_shape)\n",
    "\n",
    "# Reshaping train and test vectors to 3 dimensions\n",
    "trainDataVecs = np.reshape(trainDataVecs, (train_shape[0], 1, train_shape[1]))\n",
    "testDataVecs = np.reshape(testDataVecs, (test_shape[0], 1, test_shape[1]))\n",
    "    \n",
    "lstm_model = get_model()\n",
    "print(\"model loaded\")\n",
    "\n",
    "print(\"Training model\")\n",
    "lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=150)\n",
    "print(\"Traiing completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.9621362134889517\n"
     ]
    }
   ],
   "source": [
    "y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "\n",
    "y_pred = np.around(y_pred)\n",
    "    \n",
    "# Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----your score is out of 10---------\n",
      "1.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#train_essays = ['Gautama Buddha, popularly known as the Buddha or Lord Buddha also known as Siddhattha Gotama or Siddhārtha Gautama or Buddha Shakyamuni, was a Śramaṇa who lived in ancient India (c. 6th to 5th century BCE or century BCE). He is regarded as the founder of the world religion of Buddhism, and revered by most Buddhist schools as a savior, the Enlightened One who rediscovered an ancient path to release clinging and craving and escape the cycle of birth and rebirth. He taught for around 45 years and built a large following, both monastic and lay. His teaching is based on his insight into the arising of duḥkha (the unsatisfactoriness of clinging to impermanent states and things) and the ending of duhkha—the state called Nibbāna or Nirvana (extinguishing of the three fires).']\n",
    "train_essays = ['Covid19 taught us many things. It is possible to conduct meetings without travelling. Therefore the number of meeting have increased many times over. This has made decision making slower. ']\n",
    "\n",
    "      \n",
    "sentences = []\n",
    "    \n",
    "for essay in train_essays:\n",
    "            sentences += senteces_list(essay)\n",
    "\n",
    "clean_train_essays = []\n",
    "    \n",
    "# Generate  word vectors.\n",
    "for essay_v in train_essays:\n",
    "    clean_train_essays.append(tokenize_essay(essay_v))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, 300)\n",
    "    \n",
    "    \n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "train_shape = trainDataVecs.shape\n",
    "\n",
    "#print(train_shape)\n",
    "trainDataVecs = np.reshape(trainDataVecs, (train_shape[0], 1, train_shape[1]))\n",
    "\n",
    "    \n",
    "#lstm_model = get_model()\n",
    "#print(\"here\")\n",
    "#lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "#lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "y_pred = lstm_model.predict(trainDataVecs)\n",
    "    \n",
    "\n",
    "    \n",
    "# Round y_pred to the nearest integer.\n",
    "y_pred = np.around(y_pred)\n",
    "#print(y_pred)\n",
    "    \n",
    "final_marks = (y_pred[0][0]/60)*10\n",
    "print(\"----your score is out of 10---------\")\n",
    "print(final_marks)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
